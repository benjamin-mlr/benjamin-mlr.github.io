[{"authors":["admin"],"categories":null,"content":" I am a Postdoctoral researcher at Meta AI in the FAIR lab based in NYC.\nI completed my PhD in 2022 at Sorbonne Université and INRIA Paris in the Almanach research team.\nAI technologies are changing the way we learn, communicate and connect with each other. I am interested in scaling them to the largest number of languages. This involves designing better multimodal and multilingual models, adaptation techniques, and evaluation benchmarks.\nI\u0026rsquo;m also a former mentor and vigorous supporter of the Fatima Fellowship, a program dedicated to breaking down barriers in AI research and welcoming students from various backgrounds and origins.\n","date":1706745600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1706745600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://ben-mlr.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a Postdoctoral researcher at Meta AI in the FAIR lab based in NYC.\nI completed my PhD in 2022 at Sorbonne Université and INRIA Paris in the Almanach research team.\nAI technologies are changing the way we learn, communicate and connect with each other. I am interested in scaling them to the largest number of languages. This involves designing better multimodal and multilingual models, adaptation techniques, and evaluation benchmarks.","tags":null,"title":"Benjamin Muller","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c8e7cea3e6b2ac48c9d20082dcfc742d","permalink":"https://ben-mlr.github.io/post/slider/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/slider/","section":"post","summary":"","tags":null,"title":"","type":"post"},{"authors":null,"categories":null,"content":"The Best Way to Create the Website You Want from Markdown (or Jupyter/RStudio)\nBuild Anything with Widgets\nStar","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"85a8569f55c6c2086c14e785ef623151","permalink":"https://ben-mlr.github.io/post/hero/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/hero/","section":"post","summary":"The Best Way to Create the Website You Want from Markdown (or Jupyter/RStudio)\nBuild Anything with Widgets","tags":null,"title":"Academic","type":"post"},{"authors":null,"categories":null,"content":"Welcome to the Academic Kickstart template!\nFollow our Getting Started and Page Builder guides to easily personalize the template and then add your own content.\nFor inspiration, check out the Markdown files which power the personal demo. The easiest way to publish your new site to the internet is with Netlify.\n View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of Academic:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an Academic sticker Wear the T-shirt    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"523ab7169205f7529d3846704dedf859","permalink":"https://ben-mlr.github.io/post/demo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/demo/","section":"post","summary":"Welcome to the Academic Kickstart template!\nFollow our Getting Started and Page Builder guides to easily personalize the template and then add your own content.\nFor inspiration, check out the Markdown files which power the personal demo. The easiest way to publish your new site to the internet is with Netlify.\n View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of Academic:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an Academic sticker Wear the T-shirt    ","tags":null,"title":"Academic Kickstart","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9bde7b2b9f43447e419376962aff7c10","permalink":"https://ben-mlr.github.io/post/skills/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/skills/","section":"post","summary":"","tags":null,"title":"Skills","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a9f5bf28693a200d1a03697c1ff586a3","permalink":"https://ben-mlr.github.io/back/experience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/back/experience/","section":"back","summary":"","tags":null,"title":"Experience","type":"back"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d298054db7638d33e35f6283cc04d55b","permalink":"https://ben-mlr.github.io/post/accomplishments/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/accomplishments/","section":"post","summary":"","tags":null,"title":"Accomplish\u0026shy;ments","type":"post"},{"authors":null,"categories":null,"content":"SDFSF\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"18d742c8500e3e95a88d95021ee7f3f7","permalink":"https://ben-mlr.github.io/post/posts/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/posts/","section":"post","summary":"SDFSF","tags":null,"title":"Recent Posts","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f0c8b37e465d73f467bf3b490c273cd5","permalink":"https://ben-mlr.github.io/post/projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/projects/","section":"post","summary":"","tags":null,"title":"Projects","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"995147a03f8d91f2f66b6d06687a9677","permalink":"https://ben-mlr.github.io/post/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/people/","section":"post","summary":"","tags":null,"title":"People","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"457e61a499a0603b9ba93d7dac5234e7","permalink":"https://ben-mlr.github.io/post/resources/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/resources/","section":"post","summary":"","tags":null,"title":"Recent \u0026 Upcoming Talks","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e49c50ff7def1950f9c7f20c7bbf9921","permalink":"https://ben-mlr.github.io/post/talks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/talks/","section":"post","summary":"","tags":null,"title":"Recent \u0026 Upcoming Talks","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7fc74d3109da43174cae54f0a6e0e503","permalink":"https://ben-mlr.github.io/post/featured/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/featured/","section":"post","summary":"","tags":null,"title":"Featured Publications","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e50d1df6746968014eaaa2f617b71d4d","permalink":"https://ben-mlr.github.io/post/tags/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/post/tags/","section":"post","summary":"","tags":null,"title":"Popular Topics","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7f6dbb1ccf1f831dddabb0ef170679e0","permalink":"https://ben-mlr.github.io/back/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/back/contact/","section":"back","summary":"","tags":null,"title":"Contact","type":"back"},{"authors":["Tu Anh Nguyen","Benjamin Muller","Bokai Yu","Marta R. Costa-jussa","Maha Elbayad","Sravya Popuri","Paul-Ambroise Duquenne","Robin Algayres","Ruslan Mavlyutov","Itai Gat","Gabriel Synnaeve","Juan Pino","Benoit Sagot","Emmanuel Dupoux"],"categories":null,"content":"","date":1706745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706745600,"objectID":"439fefead4a501b1851b0efde1b5fb44","permalink":"https://ben-mlr.github.io/publication/spiritlm/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/spiritlm/","section":"publication","summary":"We introduce SPIRIT-LM, a foundation multimodal language model that freely mixes text and speech. Our model is based on a pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single set of tokens, and trained with a word-level interleaving method using a small automatically-curated speech-text parallel corpus. SPIRIT-LM comes in two versionsm a BASE version that uses speech semantic units and an EXPRESSIVE version that models expressivity using pitch and style units in addition to the semantic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that SPIRIT-LM is able to learn new tasks in a few-shot fashion across modalities (i.e. ASR, TTS, Speech Classification).","tags":["Source Themes"],"title":"SPIRIT-LM: Interleaved Spoken and Written Language Model","type":"publication"},{"authors":["Benjamin Muller","John Wieting","Jonathan Clark","Tom Kwiatkowski","Sebastian Ruder","Livio Soares","Roee Aharoni","Jonathan Herzig","Xinyi Wang"],"categories":null,"content":"","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701388800,"objectID":"05b9857a2f2cc87f06884afe3c621d2b","permalink":"https://ben-mlr.github.io/publication/xorattriqa/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/xorattriqa/","section":"publication","summary":"Trustworthy answer content is abundant in many high-resource languages and is instantly accessible through question answering systems — yet this content can be hard to access for those that do not speak these languages. The leap forward in cross-lingual modeling quality offered by generative language models offers much promise, yet their raw generations often fall short in factuality. To improve trustworthiness in these systems, a promising direction is to attribute the answer to a retrieved source, possibly in a content-rich language different from the query. Our work is the first to study attribution for cross-lingual question answering. First, we collect data in 5 languages to assess the attribution level of a state-of-the-art cross-lingual QA system. To our surprise, we find that a substantial portion of the answers is not attributable to any retrieved passages (up to 50% of answers exactly matching a gold reference) despite the system being able to attend directly to the retrieved text. Second, to address this poor attribution level, we experiment with a wide range of attribution detection techniques. We find that Natural Language Inference models and PaLM 2 fine-tuned on a very small amount of attribution data can accurately detect attribution. With these models, we improve the attribution level of a cross-lingual QA system. Overall, we show that current academic generative cross-lingual QA systems have substantial shortcomings in attribution and we build tooling to mitigate these issues.","tags":["Source Themes"],"title":"Evaluating and Modeling Attribution for Cross-Lingual Question Answering","type":"publication"},{"authors":["Benjamin Muller","Belen Alastruey","Prangthip Hansanti","Elahe Kalbassi","Christophe Ropers","Eric Michael Smith","Adina Williams","Luke Zettlemoyer","Pierre Andrews","Marta R Costa-jussà"],"categories":null,"content":"","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701388800,"objectID":"f921e16ad7f083f928eb6a8c1a752841","permalink":"https://ben-mlr.github.io/publication/gendergap/","publishdate":"2013-08-01T00:00:00Z","relpermalink":"/publication/gendergap/","section":"publication","summary":"Gender biases in language generation systems are challenging to mitigate. One possible source for these biases is gender representation disparities in the training and evaluation data. Despite recent progress in documenting this problem and many attempts at mitigating it, we still lack shared methodology and tooling to report gender representation in large datasets. Such quantitative reporting will enable further mitigation, e.g., via data augmentation. This paper describes the Gender-Gap Pipeline (for Gender-Aware Polyglot Pipeline), an automatic pipeline to characterize gender representation in large-scale datasets for 55 languages. The pipeline uses a multilingual lexicon of gendered person-nouns to quantify the gender representation in text. We showcase it to report gender representation in WMT training data and development data for the News task, confirming that current data is skewed towards masculine representation. Having unbalanced datasets may indirectly optimize our systems towards outperforming one gender over the others. We suggest introducing our gender quantification pipeline in current datasets and, ideally, modifying them toward a balanced representation.","tags":["Source Themes"],"title":"The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages","type":"publication"},{"authors":["Benjamin Muller"],"categories":null,"content":"","date":1673308800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673308800,"objectID":"198de91236a312bd68ed29bacf0af78c","permalink":"https://ben-mlr.github.io/talks/dfki/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talks/dfki/","section":"talks","summary":"","tags":["Source Themes"],"title":"DFKI Saarbrücke, MLT lab talk series","type":"talks"},{"authors":["Benjamin Muller","Luca Soldaini","Rik Koncel-Kedziorski","Eric Lind","Alessandro Moschitti"],"categories":null,"content":"","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661990400,"objectID":"50a75c1f211d181004c85884dba3c082","permalink":"https://ben-mlr.github.io/publication/crossgenqa/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/crossgenqa/","section":"publication","summary":"Open-Retrieval Generative Question Answering (GENQA) is proven to deliver high-quality, natural-sounding answers in English. In this paper, we present the first generalization of the GENQA approach for the multilingual environment. To this end, we present the GEN-TYDIQA dataset, which extends the TyDiQA evaluation data (Clark et al., 2020) with natural-sounding, well-formed answers in Arabic, Bengali, English, Japanese, and Russian. For all these languages, we show that a GENQA sequence-to-sequence-based model outperforms a state-of-the-art Answer Sentence Selection model. We also show that a multilingually-trained model competes with, and in some cases outperforms, its monolingual counterparts. Finally, we show that our system can even compete with strong baselines, even when fed with information from a variety of languages. Essentially, our system is able to answer a question in any language of our language set using information from many languages, making it the first LanguageAgnostic GENQA system","tags":["Source Themes"],"title":"Cross-Lingual GENQA: A Language-Agnostic Generative Question Answering Approach for Open-Domain Question Answering","type":"publication"},{"authors":["Benjamin Muller"],"categories":null,"content":"","date":1661817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661817600,"objectID":"8173824e932c0b93eeb01f9f73458950","permalink":"https://ben-mlr.github.io/talks/indaba/","publishdate":"2022-08-30T00:00:00Z","relpermalink":"/talks/indaba/","section":"talks","summary":"","tags":["Source Themes"],"title":"Lessons from the Camembert Model, Francophone @Indaba","type":"talks"},{"authors":["Benjamin Muller"],"categories":null,"content":"","date":1656547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656547200,"objectID":"6ed1e965ba0621c0839b1c8d185bad80","permalink":"https://ben-mlr.github.io/talks/deepvoice/","publishdate":"2022-06-30T00:00:00Z","relpermalink":"/talks/deepvoice/","section":"talks","summary":"","tags":["Source Themes"],"title":"Tutoriel: Hands-on CamemBERT: Une Introduction au Modèle CamemBERT, Deep Voice Series, IRCAM","type":"talks"},{"authors":["Benjamin Muller"],"categories":null,"content":"","date":1655856000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655856000,"objectID":"efd6aa626e35c548e8b2c0d03f2efcf9","permalink":"https://ben-mlr.github.io/talks/institut-pasteur/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talks/institut-pasteur/","section":"talks","summary":"","tags":["Source Themes"],"title":"Institut Pasteur, Imaging and Modeling lab from the Department of Computational Biology Seminar, Camembert and Beyond","type":"talks"},{"authors":["Benjamin Muller"],"categories":null,"content":"","date":1648684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648684800,"objectID":"4c3f63728d6f830c2b62ddb458331ac9","permalink":"https://ben-mlr.github.io/talks/jhu/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talks/jhu/","section":"talks","summary":"","tags":["Source Themes"],"title":"JHU CSLP Seminar, Cross-Lingual Transfer with Multilingual Language Models","type":"talks"},{"authors":["Benjamin Muller"],"categories":null,"content":"","date":1638403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638403200,"objectID":"9b0a24abea63117328754bb44f48246b","permalink":"https://ben-mlr.github.io/talks/gmu/","publishdate":"2020-09-08T00:00:00Z","relpermalink":"/talks/gmu/","section":"talks","summary":"","tags":["Source Themes"],"title":"George Mason Natural Language Processing Group , Virginia, Toward a Cross-Lingual Generative Question Answering System","type":"talks"},{"authors":["Benjamin Muller","Antonios Anastasopoulos","Benoît Sagot","Djamé Seddah"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"bc846f9ab68067e8c6f8cc6d513e01b7","permalink":"https://ben-mlr.github.io/publication/unseen-languages/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/unseen-languages/","section":"publication","summary":"Transfer learning based on pretraining language models on a large amount of raw data has become a new norm to reach state-of-the-art performance in NLP. Still, it remains unclear how this approach should be applied for unseen languages that are not covered by any available large-scale multilingual language model and for which only a small amount of raw data is generally available. In this work, by comparing multilingual and monolingual models, we show that such models behave in multiple ways on unseen languages. Some languages greatly benefit from transfer learning and behave similarly to closely related high resource languages whereas others apparently do not. Focusing on the latter, we show that this failure to transfer is largely related to the impact of the script used to write such languages. We show that transliterating those languages significantly improves the potential of large-scale multilingual language models on downstream tasks. This result provides a promising direction towards making these massively multilingual models useful for a new set of unseen languages.","tags":["Source Themes"],"title":"When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models","type":"publication"},{"authors":["Benjamin Muller","Yanai Elazar","Benoit Sagot","Djamé Seddah"],"categories":null,"content":"","date":1614556800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614556800,"objectID":"70d557e11ed85a0ce86116325fc4738a","permalink":"https://ben-mlr.github.io/publication/first-align-then-predict/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/first-align-then-predict/","section":"publication","summary":"","tags":["Source Themes"],"title":"First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT","type":"publication"},{"authors":["Benjamin Muller"],"categories":null,"content":"","date":1610668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610668800,"objectID":"b9ff3f58cc90e344e20b1ce55b7dabce","permalink":"https://ben-mlr.github.io/talks/itu-copenhagen/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talks/itu-copenhagen/","section":"talks","summary":"","tags":["Source Themes"],"title":"ITU Copenhagen, When Being unseen by mBERT is just the beginning, Handling New Languages With Multilingual Language Models","type":"talks"},{"authors":["Benjamin Muller"],"categories":null,"content":"","date":1599523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599523200,"objectID":"1a230276da2923177e99e7ed3b364b2d","permalink":"https://ben-mlr.github.io/talks/edf/","publishdate":"2020-09-08T00:00:00Z","relpermalink":"/talks/edf/","section":"talks","summary":"","tags":["Source Themes"],"title":"EDF Data Innovation Lab, Paris, CamemBERT: a Tasty French Language Model","type":"talks"},{"authors":["Louis Martin*","Benjamin Muller*","Pedro Javier Ortiz Suárez*","Yoann Dupont","Laurent Romary","Éric Villemonte de la Clergerie","Djamé Seddah","Benoît Sagot"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"f953f82f0cb8b1d1cde8149be608056a","permalink":"https://ben-mlr.github.io/publication/camembert/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/camembert/","section":"publication","summary":"Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models –in all languages except English– very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.","tags":["Source Themes"],"title":"CamemBERT: a Tasty French Language Model","type":"publication"},{"authors":["Benjamin Muller"],"categories":null,"content":"","date":1575072000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575072000,"objectID":"a8f56f3402c049e8bf813c8b2ae9942f","permalink":"https://ben-mlr.github.io/talks/bar-ilan/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talks/bar-ilan/","section":"talks","summary":"","tags":["Source Themes"],"title":"Bar Ilan University, Tel-Aviv, Transfer Learning on an Unseen North-African Arabic Dialect, December 2019","type":"talks"},{"authors":["Benjamin Muller","Benoit Sagot","Djamé Seddah"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"1ddc3e23a31768cfeda525cc00366fa0","permalink":"https://ben-mlr.github.io/publication/bert-lexnorm/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/bert-lexnorm/","section":"publication","summary":"","tags":["Source Themes"],"title":"Enhancing BERT for Lexical Normalization","type":"publication"},{"authors":["Benjamin Muller","Benoît Sagot","Djamé Seddah"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"504a937f5902408c5a9c86d446d769f0","permalink":"https://ben-mlr.github.io/publication/narabizi-adaptation/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/narabizi-adaptation/","section":"publication","summary":"Building natural language processing systems for non standardized and low resource languages is a difficult challenge. The recent success of large-scale multilingual pretrained language models provides new modeling tools to tackle this. In this work, we study the ability of multilingual language models to process an unseen dialect. We take user generated North-African Arabic as our case study, a resource-poor dialectal variety of Arabic with frequent code-mixing with French and written in Arabizi, a non-standardized transliteration of Arabic to Latin script. Focusing on two tasks, part-of-speech tagging and dependency parsing, we show in zero-shot and unsupervised adaptation scenarios that multilingual language models are able to transfer to such an unseen dialect, specifically in two extreme cases (i) across scripts, using Modern Standard Arabic as a source language, and (ii) from a distantly related language, unseen during pretraining, namely Maltese. Our results constitute the first successful transfer experiments on this dialect, paving thus the way for the development of an NLP ecosystem for resource-scarce, non-standardized and highly variable vernacular languages.","tags":["Source Themes"],"title":"Can Multilingual Language Models Transfer to an Unseen Dialect? A Case Study on North African Arabizi","type":"publication"},{"authors":null,"categories":null,"content":"September 2019\n Emotion Detection with Neural Personal Discrimination presented by Gaël Guibon [slides] [paper]  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c6625328e7b2e36b114847f299065f54","permalink":"https://ben-mlr.github.io/resources/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/","section":"","summary":"September 2019\n Emotion Detection with Neural Personal Discrimination presented by Gaël Guibon [slides] [paper]  ","tags":null,"title":"ALMANACH Study Group","type":"page"}]